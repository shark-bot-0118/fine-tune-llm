# Development Note

LoRAでのファインチューニングの実行過程で得られた知見、制約、課題などを記録しています

## 開発環境での検証結果

### 動作確認済み環境
- Python 3.12
- PyTorch 2.1+
- Apple Silicon (Mac Book pro m3)
- メモリ(ユニファイドメモリ): 36GB 

### 推奨モデルサイズ
- 1Bパラメータ: Apple Silicon (Mac Book pro m3)にて動作
- 4Bパラメータ: Apple Silicon (Mac Book pro m3)にて動作

これ以上のモデルサイズの場合、自分の実行環境ではトレーニングにかなりの時間がかかりました  
そのため、自分と同等のスペックのPCでのトレーニングの現実的なラインは4Bかと思います  
別のフレームワークや実行環境を用意した場合はその限りではありません  
一般的なスペックのPCでも0.6 ~ 1B程度であればトレーニング可能だと思います  

## 制約・注意点

### メモリ制約
トレーニングモデルのサイズによって必要とされるメモリ量が大きく変動  
下記に使用メモリの目安を記載(実測値) ※実行環境やトレーニングパラメータ、トレーニングデータ量によって大きく変動します
- **Gemma-3-1b-it**: 10GB程度
- **Gemma-3-4b-it**: 25GB程度
  
### モデル固有の制約
- **Gemma 3**: transformers>=4.46.0必須
- **tokenizer.model**: 自動生成されない場合は手動ダウンロード必要
- **EOS token**: モデルによって設定が異なる

### GGUF変換の制約
- **llama.cpp**: 必ずCMakeでビルド推奨
- **量子化**: F16→Q4への変換で2-3GB一時的にメモリ使用
- **Metal/CUDA**: 環境に応じて最適化フラグが必要

## コメント

LLMをトレーニングする場合は、現在の実行環境とモデルの性質を必ず確認してください  
特に実行環境がNvidiaかMacかで大きく変わってきます  
Macの場合はdfloat16が安定していないのか同じパラメータ、データ、モデルでも実行結果に差異があるように感じます  
今回はトレーニングしたモデルをGemmaにしていますがQwenやDeepsheekでも同じようにトレーニングが可能です  
Qwen3に関してはThinkingモードがあり、こちらのモードでは回答崩壊が起こりやすくトレーニングが難しいように思いました  
トレーニングを始める際、当然ですがトレーニングデータとモデルサイズによって適切なトレーニングパラメータを設定しなければなりません  
これが最も骨が折れる作業で、はじめはひたすらトライアンドエラーの繰り返しでした  
データが同じでもパラメータ設定が悪いと生成する回答が崩壊したり、期待通りの動きをしなかったりするのでFine-Tuningで最も苦労するポイントはおそらくパラメータ設定かと思います  
とりわけデータ量が少ない場合(400程度)、LLMの出力が不安定になる傾向があるので、データ量は極力多くした方が良いです  
最終的には400程度でもある程度機能するような設定を見つけましたが、モデルによって挙動結果は大きく異なるのでとりあえずの設定値はないような気がします(当たり前ですが)  
後述の理由であまり参考にならないかもしれませんが、以下に実際にトレーニングにかかった時間を記載します

- **Gemma-3-1b-it**: 10分
- **Gemma-3-4b-it**: 8時間  
※データ量:400件

上記を見ると**Gemma-3-4b-it**でのトレーニング時間が異常に長いですが、こちらは実行前にキャッシュクリアしておらず、裏で作業をしていたのでこれほどかかったものかと思います  
1Bのモデルが10分でおわっったので早く終わるだろうと高を括っていました...  
おそらくこちらを加味したとしても自分のMacの性能では4B程度のモデルのトレーニングには4時間程度はかかりそうです  
逆にそこそこの大きさのモデルを個人のPCでトレーニングできると考えるとそんなに時間がかかっていないようにも思えますね  
実はこのプロジェクト自体はトレーニングが話題になった半年ほど前に作っており、今回の公開に向けて内容を一部作り直しています  
トレーニング自体は興味本位ではじめ、当時はかなり苦労しましたが振り返ってみるとLLMの訳のわからない出力に何度も笑わせられて、なんだかんだ結構楽しかったと思います  
これからの時代はFine-Tuningは流行らないと思うのでトレーニング自体に需要はないかもしれませんが、自分と同じようにもし興味が湧いた方はぜひ一度自分の手でトレーニングしてみることをおすすめします！  
OpenAI Platformやほかのstudio系の自動化されたサービスではなく、自分の手で実際にチューニングしてみると意外と勉強になることが多いです！  
