# Development Note

このファイルは、LoRAファインチューニングプロジェクトの開発過程で得られた知見、制約、課題などを記録します。

## 開発環境での検証結果

### 動作確認済み環境
- Python 3.12
- PyTorch 2.1+
- Apple Silicon (Mac Book pro m3)
- メモリ(ユニファイドメモリ): 32GB 

### 推奨モデルサイズ
- 1Bパラメータ: おそらく高スペックPCでなくとも動作可能
- 4Bパラメータ: Apple Silicon (Mac Book pro m3)にて動作

## 制約・注意点

### メモリ制約
トレーニングモデルのサイズによって必要とされるメモリ量が大きく変動  
下記に使用メモリの目安を記載(実測値) ※実行環境やトレーニングパラメータ、トレーニングデータ量によって大きく変動します
- **Gemma-3-1b-it**: 10GB
- **Gemma-3-4b-it**: 25GB
  

### モデル固有の制約
- **Gemma 3**: transformers>=4.46.0必須
- **tokenizer.model**: 自動生成されない場合は手動ダウンロード必要
- **EOS token**: モデルによって設定が異なる

### GGUF変換の制約
- **llama.cpp**: 必ずCMakeでビルド推奨
- **量子化**: F16→Q4への変換で2-3GB一時的にメモリ使用
- **Metal/CUDA**: 環境に応じて最適化フラグが必要

## 開発中に発見した課題

